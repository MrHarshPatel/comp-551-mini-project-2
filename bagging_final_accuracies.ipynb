{"nbformat":4,"nbformat_minor":0,"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.7.3"},"colab":{"name":"bagging_final_accuracies.ipynb","provenance":[]}},"cells":[{"cell_type":"code","metadata":{"id":"Bi5vaUD_jj1-","colab_type":"code","colab":{}},"source":["import numpy as np\n","import time\n","import pandas as pd\n","from sklearn.datasets import fetch_20newsgroups \n","\n","from sklearn.feature_extraction.text import CountVectorizer \n","from sklearn.feature_extraction.text import TfidfTransformer \n","\n","from sklearn.linear_model import SGDClassifier \n","from sklearn.pipeline import Pipeline\n","from sklearn.svm import LinearSVC\n","from sklearn.linear_model import LogisticRegression\n","from sklearn.ensemble import AdaBoostClassifier\n","from sklearn.tree import DecisionTreeClassifier\n","from sklearn.ensemble import RandomForestClassifier\n","from sklearn.naive_bayes import MultinomialNB\n","from sklearn.ensemble import BaggingClassifier\n","\n","from sklearn.model_selection import GridSearchCV,RandomizedSearchCV\n"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"D122_BuRjj2F","colab_type":"code","colab":{}},"source":["categories = ['alt.atheism', 'soc.religion.christian', \n","              'comp.graphics', 'sci.med']\n","\n","# currently looking at all categories \n","twenty_train = fetch_20newsgroups(subset='train', shuffle=True, remove=('headers', 'footers', 'quotes'))\n","twenty_test = fetch_20newsgroups(subset='test', shuffle=True, remove=('headers', 'footers', 'quotes'))\n","\n","# For IMDB \n","train = pd.read_csv('train_IMDb.csv')\n","test = pd.read_csv('test_IMDb.csv')"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"j38TnFOWjj2J","colab_type":"code","colab":{}},"source":["class IMDB:\n","    \n","    def __init__(self,subset = 'train'):\n","        if (subset == 'train'):\n","            self.allData = train.to_numpy()\n","        else:\n","            self.allData = test.to_numpy()\n","\n","        np.random.shuffle(self.allData)\n","\n","        self.data = self.allData[:,0]\n","        self.target = self.allData[:,1]\n","        self.target=self.target.astype('int')"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"Zvvq-ptOjj2N","colab_type":"code","colab":{}},"source":["imdb_train = IMDB()\n","imdb_test = IMDB('test')"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"kdyTY6NLjj2Q","colab_type":"code","colab":{}},"source":["model_names = ['SVM', 'Logistic Regression', 'AdaBoost', 'Decision Tree', 'Random Forest']\n","\n","# just need to add the FINAL PARAMETERS!!!!!!! \n","models = [\n","    LinearSVC(random_state=0,max_iter=2000),\n","    LogisticRegression(random_state=0,max_iter=1000),\n","    BaggingClassifier(random_state=0),\n","    DecisionTreeClassifier(random_state=0),\n","    RandomForestClassifier(max_depth=2, random_state=0)\n","]"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"qwVWj_NRjj2U","colab_type":"code","colab":{}},"source":["def get_final_accuracy(train, test, estimator, model_name, parameters):\n","    \n","    adaboost = BaggingClassifier(random_state=0)\n","    \n","    start = time.time()\n","\n","    text_clf = Pipeline([('vect', CountVectorizer()),\n","                                ('tfidf', TfidfTransformer()),\n","                                ('clf', adaboost)])\n","            \n","        \n","    text_clf.set_params(clf__base_estimator=estimator)\n","    print(\"About to fit \",model_name, \" with \", text_clf.get_params)\n","    text_clf.fit(train.data, train.target)\n","    print(\"Final accuracy for: \", model_name)\n","    print(text_clf.score(test.data, test.target))\n","    print(\"Time taken: \", time.time()-start)"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"2QQZtwP7jj2a","colab_type":"text"},"source":["### Logistic Regression News"]},{"cell_type":"code","metadata":{"id":"sU4EwQv5jj2b","colab_type":"code","colab":{},"outputId":"e8a5c98f-105d-4b75-f0c4-63d2ba69bd89"},"source":["lr_parameters = {\n","    'vect__ngram_range': (1,2),\n","    'vect__stop_words': 'english',\n","    'tfidf__use_idf': True,\n","}\n","\n","lr_news = LogisticRegression(random_state=0,max_iter=1000,C=60,penalty='l2',solver='saga')\n","\n","# Just testing\n","get_final_accuracy(twenty_train, twenty_test, lr_news, 'Adaboost Logistic Reg News', lr_parameters)"],"execution_count":0,"outputs":[{"output_type":"stream","text":["About to fit  Adaboost Logistic Reg News  with  <bound method Pipeline.get_params of Pipeline(memory=None,\n","         steps=[('vect',\n","                 CountVectorizer(analyzer='word', binary=False,\n","                                 decode_error='strict',\n","                                 dtype=<class 'numpy.int64'>, encoding='utf-8',\n","                                 input='content', lowercase=True, max_df=1.0,\n","                                 max_features=None, min_df=1,\n","                                 ngram_range=(1, 1), preprocessor=None,\n","                                 stop_words=None, strip_accents=None,\n","                                 token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n","                                 tokenizer=None, vocabulary=Non...\n","                                                                     fit_intercept=True,\n","                                                                     intercept_scaling=1,\n","                                                                     l1_ratio=None,\n","                                                                     max_iter=1000,\n","                                                                     multi_class='warn',\n","                                                                     n_jobs=None,\n","                                                                     penalty='l2',\n","                                                                     random_state=0,\n","                                                                     solver='saga',\n","                                                                     tol=0.0001,\n","                                                                     verbose=0,\n","                                                                     warm_start=False),\n","                                   bootstrap=True, bootstrap_features=False,\n","                                   max_features=1.0, max_samples=1.0,\n","                                   n_estimators=10, n_jobs=None,\n","                                   oob_score=False, random_state=0, verbose=0,\n","                                   warm_start=False))],\n","         verbose=False)>\n"],"name":"stdout"},{"output_type":"stream","text":["C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:469: FutureWarning: Default multi_class will be changed to 'auto' in 0.22. Specify the multi_class option to silence this warning.\n","  \"this warning.\", FutureWarning)\n"],"name":"stderr"},{"output_type":"stream","text":["Final accuracy for:  Adaboost Logistic Reg News\n","0.6894583112055231\n","Time taken:  1169.680920124054\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"7gpYl0iPjj2f","colab_type":"text"},"source":["### Logistic Regression IMDB"]},{"cell_type":"code","metadata":{"id":"SE0Q7dx0jj2g","colab_type":"code","colab":{},"outputId":"c42891e7-83c7-4ba7-b0af-112d279b51bb"},"source":["lr_parameters = {\n","    'vect__ngram_range': (1,2),\n","    'vect__stop_words': 'english',\n","    'tfidf__use_idf': True,\n","}\n","\n","lr_imdb = LogisticRegression(random_state=0,max_iter=1000,C=55,penalty='l2',solver='saga')\n","\n","\n","# Just testing\n","get_final_accuracy(imdb_train, imdb_test, lr_imdb, 'Adaboost Logistic Reg IMDB', lr_parameters)"],"execution_count":0,"outputs":[{"output_type":"stream","text":["About to fit  Adaboost Logistic Reg IMDB  with  <bound method Pipeline.get_params of Pipeline(memory=None,\n","         steps=[('vect',\n","                 CountVectorizer(analyzer='word', binary=False,\n","                                 decode_error='strict',\n","                                 dtype=<class 'numpy.int64'>, encoding='utf-8',\n","                                 input='content', lowercase=True, max_df=1.0,\n","                                 max_features=None, min_df=1,\n","                                 ngram_range=(1, 1), preprocessor=None,\n","                                 stop_words=None, strip_accents=None,\n","                                 token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n","                                 tokenizer=None, vocabulary=Non...\n","                                                                     fit_intercept=True,\n","                                                                     intercept_scaling=1,\n","                                                                     l1_ratio=None,\n","                                                                     max_iter=1000,\n","                                                                     multi_class='warn',\n","                                                                     n_jobs=None,\n","                                                                     penalty='l2',\n","                                                                     random_state=0,\n","                                                                     solver='saga',\n","                                                                     tol=0.0001,\n","                                                                     verbose=0,\n","                                                                     warm_start=False),\n","                                   bootstrap=True, bootstrap_features=False,\n","                                   max_features=1.0, max_samples=1.0,\n","                                   n_estimators=10, n_jobs=None,\n","                                   oob_score=False, random_state=0, verbose=0,\n","                                   warm_start=False))],\n","         verbose=False)>\n","Final accuracy for:  Adaboost Logistic Reg IMDB\n","0.87984\n","Time taken:  173.71956992149353\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"EtAkCW8Cjj2k","colab_type":"text"},"source":["### SVM News"]},{"cell_type":"code","metadata":{"id":"upOf9h7Jjj2l","colab_type":"code","colab":{},"outputId":"50ad17f6-98f7-488f-a131-70a5a6129693"},"source":["svm_parameters = {\n","    'vect__ngram_range': (1,2),\n","    'vect__stop_words': 'english',\n","    'tfidf__use_idf': True,\n","    'clf__algorithm': 'SAMME'\n","}\n","\n","svm_news = LinearSVC(random_state=0,max_iter=2000, C=75,loss='squared_hinge',penalty='l2')\n","\n","\n","# Just testing\n","get_final_accuracy(twenty_train, twenty_test, svm_news, 'Adaboost Linear SVM News', svm_parameters)"],"execution_count":0,"outputs":[{"output_type":"stream","text":["About to fit  Adaboost Linear SVM News  with  <bound method Pipeline.get_params of Pipeline(memory=None,\n","         steps=[('vect',\n","                 CountVectorizer(analyzer='word', binary=False,\n","                                 decode_error='strict',\n","                                 dtype=<class 'numpy.int64'>, encoding='utf-8',\n","                                 input='content', lowercase=True, max_df=1.0,\n","                                 max_features=None, min_df=1,\n","                                 ngram_range=(1, 1), preprocessor=None,\n","                                 stop_words=None, strip_accents=None,\n","                                 token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n","                                 tokenizer=None, vocabulary=Non...\n","                 BaggingClassifier(base_estimator=LinearSVC(C=75,\n","                                                            class_weight=None,\n","                                                            dual=True,\n","                                                            fit_intercept=True,\n","                                                            intercept_scaling=1,\n","                                                            loss='squared_hinge',\n","                                                            max_iter=2000,\n","                                                            multi_class='ovr',\n","                                                            penalty='l2',\n","                                                            random_state=0,\n","                                                            tol=0.0001,\n","                                                            verbose=0),\n","                                   bootstrap=True, bootstrap_features=False,\n","                                   max_features=1.0, max_samples=1.0,\n","                                   n_estimators=10, n_jobs=None,\n","                                   oob_score=False, random_state=0, verbose=0,\n","                                   warm_start=False))],\n","         verbose=False)>\n"],"name":"stdout"},{"output_type":"stream","text":["C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\svm\\base.py:929: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n","  \"the number of iterations.\", ConvergenceWarning)\n"],"name":"stderr"},{"output_type":"stream","text":["Final accuracy for:  Adaboost Linear SVM News\n","0.6642326075411578\n","Time taken:  431.23776054382324\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"VEHuTSyujj2o","colab_type":"text"},"source":["### SVM IMDb\n"]},{"cell_type":"code","metadata":{"id":"ECqBKMgrjj2p","colab_type":"code","colab":{},"outputId":"5860df9a-4851-45a9-8273-7e8155951c0c"},"source":["svm_parameters = {\n","    'vect__ngram_range': (1,2),\n","    'vect__stop_words': 'english',\n","    'tfidf__use_idf': True,\n","    'clf__algorithm': 'SAMME'\n","}\n","\n","svm_imdb = LinearSVC(random_state=0,max_iter=2000, C=5,loss='squared_hinge',penalty='l2')\n","\n","# Just testing\n","get_final_accuracy(imdb_train, imdb_test, svm_imdb, 'Adaboost Linear SVM IMDb', svm_parameters)"],"execution_count":0,"outputs":[{"output_type":"stream","text":["About to fit  Adaboost Linear SVM IMDb  with  <bound method Pipeline.get_params of Pipeline(memory=None,\n","         steps=[('vect',\n","                 CountVectorizer(analyzer='word', binary=False,\n","                                 decode_error='strict',\n","                                 dtype=<class 'numpy.int64'>, encoding='utf-8',\n","                                 input='content', lowercase=True, max_df=1.0,\n","                                 max_features=None, min_df=1,\n","                                 ngram_range=(1, 1), preprocessor=None,\n","                                 stop_words=None, strip_accents=None,\n","                                 token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n","                                 tokenizer=None, vocabulary=Non...\n","                 BaggingClassifier(base_estimator=LinearSVC(C=5,\n","                                                            class_weight=None,\n","                                                            dual=True,\n","                                                            fit_intercept=True,\n","                                                            intercept_scaling=1,\n","                                                            loss='squared_hinge',\n","                                                            max_iter=2000,\n","                                                            multi_class='ovr',\n","                                                            penalty='l2',\n","                                                            random_state=0,\n","                                                            tol=0.0001,\n","                                                            verbose=0),\n","                                   bootstrap=True, bootstrap_features=False,\n","                                   max_features=1.0, max_samples=1.0,\n","                                   n_estimators=10, n_jobs=None,\n","                                   oob_score=False, random_state=0, verbose=0,\n","                                   warm_start=False))],\n","         verbose=False)>\n","Final accuracy for:  Adaboost Linear SVM IMDb\n","0.86028\n","Time taken:  25.05803370475769\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"V2gwAoXUjj2t","colab_type":"text"},"source":["### Decision Tree News\n"]},{"cell_type":"code","metadata":{"id":"QtGxYvB5jj2u","colab_type":"code","colab":{},"outputId":"62557d79-39af-4053-acca-41c94889e83a"},"source":["dt_parameters = {\n","    'vect__ngram_range': (1,2),\n","    'vect__stop_words': 'english',\n","    'tfidf__use_idf': True,\n","}\n","        \n","dt_news = DecisionTreeClassifier(random_state=0, max_depth=15, max_features=None, min_impurity_decrease=0.0015)\n","\n","# Just testing\n","get_final_accuracy(twenty_train, twenty_test, dt_news, 'Adaboost Decision Tree News', dt_parameters)"],"execution_count":0,"outputs":[{"output_type":"stream","text":["About to fit  Adaboost Decision Tree News  with  <bound method Pipeline.get_params of Pipeline(memory=None,\n","         steps=[('vect',\n","                 CountVectorizer(analyzer='word', binary=False,\n","                                 decode_error='strict',\n","                                 dtype=<class 'numpy.int64'>, encoding='utf-8',\n","                                 input='content', lowercase=True, max_df=1.0,\n","                                 max_features=None, min_df=1,\n","                                 ngram_range=(1, 1), preprocessor=None,\n","                                 stop_words=None, strip_accents=None,\n","                                 token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n","                                 tokenizer=None, vocabulary=Non...\n","                                                                         max_leaf_nodes=None,\n","                                                                         min_impurity_decrease=0.0015,\n","                                                                         min_impurity_split=None,\n","                                                                         min_samples_leaf=1,\n","                                                                         min_samples_split=2,\n","                                                                         min_weight_fraction_leaf=0.0,\n","                                                                         presort=False,\n","                                                                         random_state=0,\n","                                                                         splitter='best'),\n","                                   bootstrap=True, bootstrap_features=False,\n","                                   max_features=1.0, max_samples=1.0,\n","                                   n_estimators=10, n_jobs=None,\n","                                   oob_score=False, random_state=0, verbose=0,\n","                                   warm_start=False))],\n","         verbose=False)>\n","Final accuracy for:  Adaboost Decision Tree News\n","0.280801911842804\n","Time taken:  17.753427267074585\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"A_45BfJ-jj2x","colab_type":"text"},"source":["### Decision Tree IMDb"]},{"cell_type":"code","metadata":{"id":"qjdtNqhgjj2y","colab_type":"code","colab":{},"outputId":"132304d8-f57c-4b81-aff5-cb203b0721c6"},"source":["dt_parameters = {\n","    'vect__ngram_range': (1,2),\n","    'vect__stop_words': 'english',\n","    'tfidf__use_idf': True,\n","}\n","\n","dt_imdb = DecisionTreeClassifier(random_state=0, max_depth=40, max_features=None, min_impurity_decrease=0.0005)\n","\n","# Just testing\n","get_final_accuracy(imdb_train, imdb_test, dt_imdb, 'Adaboost Decision Tree IMDb', dt_parameters)"],"execution_count":0,"outputs":[{"output_type":"stream","text":["About to fit  Adaboost Decision Tree IMDb  with  <bound method Pipeline.get_params of Pipeline(memory=None,\n","         steps=[('vect',\n","                 CountVectorizer(analyzer='word', binary=False,\n","                                 decode_error='strict',\n","                                 dtype=<class 'numpy.int64'>, encoding='utf-8',\n","                                 input='content', lowercase=True, max_df=1.0,\n","                                 max_features=None, min_df=1,\n","                                 ngram_range=(1, 1), preprocessor=None,\n","                                 stop_words=None, strip_accents=None,\n","                                 token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n","                                 tokenizer=None, vocabulary=Non...\n","                                                                         max_leaf_nodes=None,\n","                                                                         min_impurity_decrease=0.0005,\n","                                                                         min_impurity_split=None,\n","                                                                         min_samples_leaf=1,\n","                                                                         min_samples_split=2,\n","                                                                         min_weight_fraction_leaf=0.0,\n","                                                                         presort=False,\n","                                                                         random_state=0,\n","                                                                         splitter='best'),\n","                                   bootstrap=True, bootstrap_features=False,\n","                                   max_features=1.0, max_samples=1.0,\n","                                   n_estimators=10, n_jobs=None,\n","                                   oob_score=False, random_state=0, verbose=0,\n","                                   warm_start=False))],\n","         verbose=False)>\n","Final accuracy for:  Adaboost Decision Tree IMDb\n","0.77348\n","Time taken:  88.74706673622131\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"zVWmVAWQjj21","colab_type":"text"},"source":["### Random Forest News"]},{"cell_type":"code","metadata":{"id":"kNmYko6vjj22","colab_type":"code","colab":{},"outputId":"d0477f06-abe5-4bd4-c4be-f96dfe2facc0"},"source":["rf_parameters = {\n","    'vect__ngram_range': (1,2),\n","    'vect__stop_words': 'english',\n","    'tfidf__use_idf': True,\n","\n","}\n","\n","#     'clf__bootstrap': False,\n","#     'clf__max_depth': None,\n","#     'clf__max_features': 'auto',\n","#     'clf__min_samples_split': 10, \n","#     'clf__n_estimators': 800\n","        \n","rf_news = RandomForestClassifier(max_depth=None, random_state=0, bootstrap=False, max_features='auto', min_samples_split=10,n_estimators=800)\n","\n","# Just testing\n","get_final_accuracy(twenty_train, twenty_test, rf_news, 'Random Forest News', rf_parameters)"],"execution_count":0,"outputs":[{"output_type":"stream","text":["About to fit  Random Forest News  with  <bound method Pipeline.get_params of Pipeline(memory=None,\n","         steps=[('vect',\n","                 CountVectorizer(analyzer='word', binary=False,\n","                                 decode_error='strict',\n","                                 dtype=<class 'numpy.int64'>, encoding='utf-8',\n","                                 input='content', lowercase=True, max_df=1.0,\n","                                 max_features=None, min_df=1,\n","                                 ngram_range=(1, 1), preprocessor=None,\n","                                 stop_words=None, strip_accents=None,\n","                                 token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n","                                 tokenizer=None, vocabulary=Non...\n","                                                                         min_impurity_split=None,\n","                                                                         min_samples_leaf=1,\n","                                                                         min_samples_split=10,\n","                                                                         min_weight_fraction_leaf=0.0,\n","                                                                         n_estimators=800,\n","                                                                         n_jobs=None,\n","                                                                         oob_score=False,\n","                                                                         random_state=0,\n","                                                                         verbose=0,\n","                                                                         warm_start=False),\n","                                   bootstrap=True, bootstrap_features=False,\n","                                   max_features=1.0, max_samples=1.0,\n","                                   n_estimators=10, n_jobs=None,\n","                                   oob_score=False, random_state=0, verbose=0,\n","                                   warm_start=False))],\n","         verbose=False)>\n","Final accuracy for:  Random Forest News\n","0.6351566648964418\n","Time taken:  1856.8537302017212\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"yu_RyU3ijj27","colab_type":"text"},"source":["### Random Forest IMDb"]},{"cell_type":"code","metadata":{"id":"KJXHJ-MRjj28","colab_type":"code","colab":{}},"source":[""],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"cxsehqCojj2_","colab_type":"text"},"source":["### Naive Bayes News"]},{"cell_type":"code","metadata":{"id":"qgrzGoB1jj3A","colab_type":"code","colab":{},"outputId":"50baf2fe-165a-484a-aa26-92a744a756ac"},"source":["nb_parameters = {\n","    'vect__ngram_range': (1,2),\n","    'vect__stop_words': 'english',\n","    'tfidf__use_idf': True,\n","}\n","\n","get_final_accuracy(twenty_train, twenty_test, MultinomialNB(), 'AdaBoost Naive Bayes News', nb_parameters)"],"execution_count":0,"outputs":[{"output_type":"stream","text":["About to fit  AdaBoost Naive Bayes News  with  <bound method Pipeline.get_params of Pipeline(memory=None,\n","         steps=[('vect',\n","                 CountVectorizer(analyzer='word', binary=False,\n","                                 decode_error='strict',\n","                                 dtype=<class 'numpy.int64'>, encoding='utf-8',\n","                                 input='content', lowercase=True, max_df=1.0,\n","                                 max_features=None, min_df=1,\n","                                 ngram_range=(1, 1), preprocessor=None,\n","                                 stop_words=None, strip_accents=None,\n","                                 token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n","                                 tokenizer=None, vocabulary=Non...\n","                 TfidfTransformer(norm='l2', smooth_idf=True,\n","                                  sublinear_tf=False, use_idf=True)),\n","                ('clf',\n","                 BaggingClassifier(base_estimator=MultinomialNB(alpha=1.0,\n","                                                                class_prior=None,\n","                                                                fit_prior=True),\n","                                   bootstrap=True, bootstrap_features=False,\n","                                   max_features=1.0, max_samples=1.0,\n","                                   n_estimators=10, n_jobs=None,\n","                                   oob_score=False, random_state=0, verbose=0,\n","                                   warm_start=False))],\n","         verbose=False)>\n","Final accuracy for:  AdaBoost Naive Bayes News\n","0.6011683483802442\n","Time taken:  4.9948811531066895\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"3_sr-KrVjj3C","colab_type":"text"},"source":["### Naive Bayes IMDb"]},{"cell_type":"code","metadata":{"id":"oLzc2kP6jj3D","colab_type":"code","colab":{},"outputId":"689af2f7-b7f6-4e9b-e659-b368fa44cb39"},"source":["nb_parameters = {\n","    'vect__ngram_range': (1,2),\n","    'vect__stop_words': 'english',\n","    'tfidf__use_idf': True,\n","}\n","\n","\n","get_final_accuracy(imdb_train, imdb_test, MultinomialNB(), 'AdaBoost Naive Bayes News', nb_parameters)"],"execution_count":0,"outputs":[{"output_type":"stream","text":["About to fit  AdaBoost Naive Bayes News  with  <bound method Pipeline.get_params of Pipeline(memory=None,\n","         steps=[('vect',\n","                 CountVectorizer(analyzer='word', binary=False,\n","                                 decode_error='strict',\n","                                 dtype=<class 'numpy.int64'>, encoding='utf-8',\n","                                 input='content', lowercase=True, max_df=1.0,\n","                                 max_features=None, min_df=1,\n","                                 ngram_range=(1, 1), preprocessor=None,\n","                                 stop_words=None, strip_accents=None,\n","                                 token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n","                                 tokenizer=None, vocabulary=Non...\n","                 TfidfTransformer(norm='l2', smooth_idf=True,\n","                                  sublinear_tf=False, use_idf=True)),\n","                ('clf',\n","                 BaggingClassifier(base_estimator=MultinomialNB(alpha=1.0,\n","                                                                class_prior=None,\n","                                                                fit_prior=True),\n","                                   bootstrap=True, bootstrap_features=False,\n","                                   max_features=1.0, max_samples=1.0,\n","                                   n_estimators=10, n_jobs=None,\n","                                   oob_score=False, random_state=0, verbose=0,\n","                                   warm_start=False))],\n","         verbose=False)>\n","Final accuracy for:  AdaBoost Naive Bayes News\n","0.83028\n","Time taken:  11.770794868469238\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"6gtEsKpejj3G","colab_type":"code","colab":{}},"source":[""],"execution_count":0,"outputs":[]}]}